{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sequential model is a linear stack of layers.\n",
    "# You can create a Sequential model by passing a list of layer instances to the constructor:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also simply add layers via the .add() method\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "# For custom metrics\n",
    "import keras.backend as k\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy', mean_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 421us/step - loss: 0.7080 - acc: 0.5130\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.6989 - acc: 0.5180\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.6912 - acc: 0.5480\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.6870 - acc: 0.5370\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.6816 - acc: 0.5730\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.6754 - acc: 0.5720\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.6741 - acc: 0.5780\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 0.6696 - acc: 0.5940\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.6649 - acc: 0.6130\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.6600 - acc: 0.6360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b391ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# For a single-input model with 2 classes (binary classification)\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 307us/step - loss: 2.3497 - acc: 0.1150\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 2.3062 - acc: 0.1250\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 2.2905 - acc: 0.1310\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 2.2804 - acc: 0.1340\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 2.2710 - acc: 0.1470\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 2.2610 - acc: 0.1590\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 2.2493 - acc: 0.1680\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 2.2427 - acc: 0.1680\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 2.2312 - acc: 0.1780\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 2.2233 - acc: 0.1820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b9a9ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 10 classes (categorical classification):\n",
    "import keras\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting started 30 Seconds to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vfrom keras.models import Sequential\n",
    "# model = Sequential()\n",
    "\n",
    "# Stacking layers is as easy as .add():\n",
    "#from keras.layers import Dense\n",
    "#model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "#model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Once your model looks good, configure its learning process with.compile()\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#             optimizer='sgd',\n",
    "#             metrics=['accuracy'])\n",
    "\n",
    "# If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).\n",
    "#model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#             optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "\n",
    "# You can now iterate on your training data in batches:\n",
    "# x_train and y_train are numpy arrays --just like in the Scikit-Learn API.\n",
    "#model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Alternatively, you can feed batches to your model manually:\n",
    "# model.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "# Evaluate your performance in one line:\n",
    "#loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
    "\n",
    "# Or generate predictions on new data:\n",
    "#class = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 2.3842 - acc: 0.1270\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 2.3515 - acc: 0.0870\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3365 - acc: 0.0970\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3383 - acc: 0.0980\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3275 - acc: 0.1020\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3329 - acc: 0.1020\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3183 - acc: 0.1090\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3018 - acc: 0.1190\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3147 - acc: 0.1140\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3039 - acc: 0.1240\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3128 - acc: 0.1050\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3106 - acc: 0.1110\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3105 - acc: 0.1120\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3035 - acc: 0.1080\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 2.3028 - acc: 0.1030\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 2.2990 - acc: 0.1010\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 2.2986 - acc: 0.1070\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 2.2989 - acc: 0.1150\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 2.2994 - acc: 0.1210\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 2.2949 - acc: 0.1140\n",
      "100/100 [==============================] - 0s 967us/step\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron (MLP) for Multi-class softmax classification:\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate Dummy Data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected Layer with 64 hidden unites\n",
    "# In the first Layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional, vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=sgd,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         epochs=20,\n",
    "         batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.7205 - acc: 0.5220\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.7161 - acc: 0.5120\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7080 - acc: 0.5190\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7101 - acc: 0.4940\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.7073 - acc: 0.5060\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7048 - acc: 0.5100\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7057 - acc: 0.4920\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6985 - acc: 0.5290\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6934 - acc: 0.5250\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.7011 - acc: 0.4920\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6974 - acc: 0.5080\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6949 - acc: 0.5230\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6936 - acc: 0.5180\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6968 - acc: 0.5090\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6908 - acc: 0.5360\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6950 - acc: 0.5040\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6900 - acc: 0.5180\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6918 - acc: 0.5090\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6943 - acc: 0.5070\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6873 - acc: 0.5400\n",
      "1000/1000 [==============================] - 0s 142us/step\n"
     ]
    }
   ],
   "source": [
    "# MLP for binary classification\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.random((1000, 20))\n",
    "y_test = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 2.3464\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 2.3126\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 2.2879\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 2.2723\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 2.2856\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 2.3079\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 2.3047\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 2.2968\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 2.2938\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 2.2927\n",
      "20/20 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "# VGG-Like Convnet:\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Denerate dummy data\n",
    "x_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# This applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_2_input to have 2 dimensions, but got array with shape (100, 100, 100, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-bb6946ebf46b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataScience/TensorFlow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataScience/TensorFlow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataScience/TensorFlow/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_2_input to have 2 dimensions, but got array with shape (100, 100, 100, 3)"
     ]
    }
   ],
   "source": [
    "# Sequence classification with LSTM\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "max_features = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=256))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 11.5265 - acc: 0.1000 - val_loss: 11.5632 - val_acc: 0.1100\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 421us/step - loss: 11.5237 - acc: 0.1120 - val_loss: 11.5643 - val_acc: 0.1400\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 410us/step - loss: 11.5238 - acc: 0.1080 - val_loss: 11.5631 - val_acc: 0.1200\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 413us/step - loss: 11.5232 - acc: 0.1110 - val_loss: 11.5609 - val_acc: 0.1000\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 412us/step - loss: 11.5228 - acc: 0.0990 - val_loss: 11.5645 - val_acc: 0.0700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1271d4d30>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence Classification with 1D convolutions:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "\n",
    "# Expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "              input_shape=(timesteps, data_dim))) # Returns a sequence of vectors of dimentsion\n",
    "model.add(LSTM(32, return_sequences=True))  # Returs a sequence of vectors of dimenstion 32\n",
    "model.add(LSTM(32)) # Returns a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer = 'rmsprop',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "# Generate dummy trining data\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         batch_size = 64, epochs=5,\n",
    "         validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, validate on 96 samples\n",
      "Epoch 1/5\n",
      "320/320 [==============================] - 3s 8ms/step - loss: 11.4566 - acc: 0.0844 - val_loss: 11.6345 - val_acc: 0.1354\n",
      "Epoch 2/5\n",
      "320/320 [==============================] - 0s 575us/step - loss: 11.4530 - acc: 0.1094 - val_loss: 11.6341 - val_acc: 0.1354\n",
      "Epoch 3/5\n",
      "320/320 [==============================] - 0s 578us/step - loss: 11.4517 - acc: 0.1125 - val_loss: 11.6339 - val_acc: 0.1458\n",
      "Epoch 4/5\n",
      "320/320 [==============================] - 0s 572us/step - loss: 11.4508 - acc: 0.1219 - val_loss: 11.6337 - val_acc: 0.1562\n",
      "Epoch 5/5\n",
      "320/320 [==============================] - 0s 575us/step - loss: 11.4500 - acc: 0.1250 - val_loss: 11.6335 - val_acc: 0.1562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120fb4630>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same stacked LSTM model, rendered 'stateful'\n",
    "# A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "              batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "model.add(LSTM(32, stateful=True))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer = 'rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "y_train = np.random.random((batch_size * 10, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((batch_size * 3, timesteps, data_dim))\n",
    "y_val = np.random.random((batch_size * 3, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         batch_size=batch_size, epochs=5, shuffle=False,\n",
    "         validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting started with the Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have shape (784,) but got array with shape (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f1a3fa884f59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# starts training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/DataScience/TensorFlow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataScience/TensorFlow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataScience/TensorFlow/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_2 to have shape (784,) but got array with shape (100,)"
     ]
    }
   ],
   "source": [
    "# First Example: A densely-connected Network\n",
    "\n",
    "# The Sequential model is probably a better choice to implement such a network, but it helps to start with something really simple.\n",
    "    # A layer instance is callable (on a tensor), and it returns a tensor\n",
    "    # Input tensor(s) and output tensor(s) can then be used to define a Model\n",
    "    # Such a model can be trained just like Keras Sequential models.\n",
    "    \n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
